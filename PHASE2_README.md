# Phase 2: Content Optimization with AI

Complete! Now articles can be optimized using Google search + LLM.

## ğŸ¯ What Phase 2 Does:

1. âœ… Fetches articles from your API
2. âœ… Searches article title on Google
3. âœ… Scrapes top 2 ranking articles
4. âœ… Uses OpenAI to optimize content
5. âœ… Publishes optimized version with citations
6. âœ… Links back to original article

---

## ğŸ”§ Setup

### 1. Get Groq API Key (Required) - FREE!

1. Go to https://console.groq.com/keys
2. Create account / Sign in with Google
3. Click "Create API Key"
4. Copy your API key

**Why Groq?**
- âš¡ 10x faster than OpenAI
- ğŸ†“ Generous free tier
- ğŸ¯ High quality (uses Llama 3.3 70B)

### 2. Add to `.env` file:

```env
GROQ_API_KEY=gsk_xxxxxxxxxxxxxxxxxxxxx
```

### 3. Google Search (Optional)

**Option A: Use Google scraping** (Free, no setup needed)
- Already works out of the box!

**Option B: Use Google Custom Search API** (More reliable)
1. Go to https://developers.google.com/custom-search
2. Create a Custom Search Engine
3. Get API Key and Search Engine ID
4. Add to `.env`:

```env
GOOGLE_API_KEY=your_api_key
GOOGLE_SEARCH_ENGINE_ID=your_cx_id
```

---

## ğŸš€ Usage

### Make sure server is running:
```bash
npm start
```

### Option 1: Optimize All Articles
```bash
npm run optimize
```

This will:
- Find all non-optimized articles
- Process each one automatically
- Create optimized versions

### Option 2: Optimize Specific Article
```bash
node scripts/contentOptimizer.js ARTICLE_ID
```

Replace `ARTICLE_ID` with actual MongoDB ID from your database.

---

## ğŸ“Š What Gets Created:

For each original article, it creates:

**Optimized Article:**
- âœ… Better formatting (headings, structure)
- âœ… Enhanced content quality
- âœ… SEO optimized
- âœ… Similar style to top-ranking articles
- âœ… Citations at bottom
- âœ… Linked to original article

**Fields:**
- `isOptimized: true`
- `originalArticleId: [original_article_id]`
- `references: [array of source articles]`

---

## ğŸ§ª Testing Phase 2

### 1. Make sure you have articles:
```bash
npm run scrape
```

### 2. Start server:
```bash
npm start
```

### 3. Run optimizer:
```bash
npm run optimize
```

### 4. Check results:
```bash
# Get all optimized articles
curl "http://localhost:5000/api/articles?isOptimized=true"

# Get original articles
curl "http://localhost:5000/api/articles?isOptimized=false"
```

---

## ğŸ” Pipeline Flow:

```
Original Article
      â†“
Search Google for title
      â†“
Get top 2 results
      â†“
Scrape their content
      â†“
Send to OpenAI (GPT-4)
      â†“
Generate optimized version
      â†“
Add citations
      â†“
Publish via API
      â†“
Optimized Article âœ¨
```

---

## âš™ï¸ Files Created:

- `utils/googleSearch.js` - Google search (API + scraping)
- `utils/articleScraper.js` - Extract article content
- `utils/contentOptimizer.js` - OpenAI integration
- `scripts/contentOptimizer.js` - Main pipeline script

---

## ğŸ’¡ Tips:

1. **OpenAI Cost**: Uses `gpt-4o-mini` (cheap model)
   - ~$0.01 per article
   - Can switch to `gpt-3.5-turbo` for even cheaper

2. **Rate Limits**: 
   - 5 second delay between articles
   - Prevents API throttling

3. **Google Scraping**:
   - Works without API keys
   - Falls back automatically if API fails

---

## âœ… Phase 2 Complete!

Ready for Phase 3 (React Frontend)? Let me know! ğŸš€
